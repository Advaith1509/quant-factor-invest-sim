{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "35b27659",
      "metadata": {
        "id": "35b27659"
      },
      "source": [
        "# ***Factor Scoring***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0f1bd19a",
      "metadata": {
        "id": "0f1bd19a",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "xWH8lA6j9SSC",
      "metadata": {
        "id": "xWH8lA6j9SSC"
      },
      "outputs": [],
      "source": [
        "fundamentals = pd.read_csv(\"fundamentals_clean.csv\")\n",
        "prices = pd.read_csv(\"daily_prices_clean.csv\")\n",
        "prices_split_adjusted = pd.read_csv(\"daily_adjusted_prices_clean.csv\")\n",
        "securities = pd.read_csv(\"securities_clean.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m2Ln0osLDTPe",
      "metadata": {
        "id": "m2Ln0osLDTPe"
      },
      "source": [
        "## ***Value Factor***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6rssgzfNDeMP",
      "metadata": {
        "id": "6rssgzfNDeMP"
      },
      "source": [
        "***Idea:*** Buy undervalued stocks, ones that are cheap relative to their fundamentals.\n",
        "\n",
        "***Key Metrics:***\n",
        "  * **P/E Ratio (Price to Earnings) -** Lower is better (cheap earnings).\n",
        "  * **P/B Ratio (Price to Book) -** Lower suggests undervaluation.\n",
        "  * **EV/EBITDA or EV/Sales -** Adjusted for debt and cash.\n",
        "\n",
        "> We'll rank stocks based on a composite value score (e.g., average rank of P/E, P/B, EV/EBITDA).\n",
        "\n",
        "*(EV - Enterprise Value)\n",
        ", (EBITDA - Earnings Before Interest, Taxes, Depreciation, and Amortization*)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T6qNHNZ9-uo-",
      "metadata": {
        "id": "T6qNHNZ9-uo-"
      },
      "source": [
        "* Price-to-Earnings (P/E): `close / eps`\n",
        "* Price-to-Book (P/B): `close / (total_assets - total_liabilities)`\n",
        "* Dividend Yield: `dividend_per_share / close`\n",
        "* PEG Ratio: `pe / earnings_growth_rate`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "HIPFYyiCHO9P",
      "metadata": {
        "id": "HIPFYyiCHO9P"
      },
      "outputs": [],
      "source": [
        "def calculate_value_factor(fundamentals_df, prices_df):\n",
        "    value_df = pd.merge(fundamentals_df, prices_df, left_on='ticker_symbol', right_on='symbol', how='left')\n",
        "\n",
        "    value_df['pe'] = value_df['close'] / value_df['earnings_per_share'].replace(0, np.nan)\n",
        "    value_df['pb'] = value_df['close'] / (value_df['total_assets'] - value_df['total_liabilities']).replace(0, np.nan)\n",
        "    value_df['earnings_growth'] = value_df.groupby('symbol')['earnings_per_share'].pct_change(periods=4)\n",
        "\n",
        "    for metric in ['pe', 'pb']:\n",
        "        # lower the better hence, dividing it by 1\n",
        "        value_df[metric + '_norm'] = 1 / value_df.groupby('date')[metric].transform(\n",
        "            lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "        )\n",
        "\n",
        "    value_df['value_score'] = value_df[['pe_norm', 'pb_norm']].mean(axis=1)\n",
        "\n",
        "    # Ensure date is datetime64[ns]\n",
        "    value_df['date'] = pd.to_datetime(value_df['date'])\n",
        "\n",
        "    return value_df[['symbol', 'date', 'value_score']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b_h5MsmFMzX",
      "metadata": {
        "id": "3b_h5MsmFMzX"
      },
      "source": [
        "## ***Momentum Factor***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R7Y2BiAfFOcF",
      "metadata": {
        "id": "R7Y2BiAfFOcF"
      },
      "source": [
        "***Idea:*** Stocks that performed well recently tend to keep doing well in the short term.\n",
        "\n",
        "***Key Metrics:***\n",
        "  * **3M, 6M -** 3-month or 6-month price return. Higher is better.\n",
        "  * **Relative Strength Index (RSI) -** For overbought/oversold signals.\n",
        "\n",
        "> We'll sort stocks by past 12-month returns and select the top decile.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1iK93mjXBoQF",
      "metadata": {
        "id": "1iK93mjXBoQF"
      },
      "source": [
        "* 3-Month Momentum: `(current_close - close_3mo_ago) / close_3mo_ago`\n",
        "* 6-Month Momentum: `(current_close - close_6mo_ago) / close_6mo_ago`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "pPfYExLCAjXQ",
      "metadata": {
        "id": "pPfYExLCAjXQ"
      },
      "outputs": [],
      "source": [
        "def calculate_momentum_factor(prices_df):\n",
        "    momentum_df = prices_df.copy()\n",
        "\n",
        "    # 3M(=63 days) and 6M(126 days) Returns\n",
        "    momentum_df['3m_return'] = momentum_df.groupby('symbol')['close'].pct_change(periods=63)\n",
        "    momentum_df['6m_return'] = momentum_df.groupby('symbol')['close'].pct_change(periods=126)\n",
        "\n",
        "    # RSI (Relative Strength Index)\n",
        "    delta = momentum_df.groupby('symbol')['close'].diff()\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    avg_gain = gain.rolling(window=14).mean()\n",
        "    avg_loss = loss.rolling(window=14).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-8)\n",
        "    momentum_df['rsi'] = 100 - (100 / (1 + rs + 1e-8))\n",
        "\n",
        "    # Normalize and combine the metrics\n",
        "    momentum_metrics = ['3m_return', '6m_return', 'rsi']\n",
        "    for metric in momentum_metrics:\n",
        "        # As higher momentum is better, we don't resiprocate it.\n",
        "        momentum_df[metric + '_norm'] = momentum_df.groupby('date')[metric].transform(\n",
        "            lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "        )\n",
        "\n",
        "    momentum_df['momentum_score'] = momentum_df[[m + '_norm' for m in momentum_metrics]].mean(axis=1)\n",
        "    momentum_df['date'] = pd.to_datetime(momentum_df['date'])\n",
        "    return momentum_df[['symbol', 'date', 'momentum_score']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VLBuhwJhHXqi",
      "metadata": {
        "id": "VLBuhwJhHXqi"
      },
      "source": [
        "## ***Quality Factor***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cs1v6GXRHP7u",
      "metadata": {
        "id": "Cs1v6GXRHP7u"
      },
      "source": [
        "***Idea:***  Invest in financially healthy and efficient companies.\n",
        "\n",
        "***Key Metrics:***\n",
        "  * **ROE (Return on Equity) -** Higher = better efficiency.\n",
        "  * **Debt-to-Equity Ratio -** Lower = Less Financial Risk.\n",
        "  * **Profit Margin**\n",
        "\n",
        "*High-quality companies are more resilient to downturns and are often underpriced due to market inefficiencies and hence, Quality is important.*\n",
        "\n",
        "> We'll create a composite quality score from ROE, D/E, and margin stability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "znRVt_rnB7HH",
      "metadata": {
        "id": "znRVt_rnB7HH"
      },
      "source": [
        "* Return on Assets (ROA): `net_income / total_assets`\n",
        "* Asset Turnover Change: `(current_revenue/assets - prev_revenue/assets)`\n",
        "* Accruals: `(net_income - operating_cash_flow) / total_assets`\n",
        "* Leverage: `operating_cash_flow / total_debt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cuE4xrYvIMLs",
      "metadata": {
        "id": "cuE4xrYvIMLs"
      },
      "outputs": [],
      "source": [
        "def calculate_quality_factor(fundamentals_df):\n",
        "    qual_df = fundamentals_df.copy()\n",
        "\n",
        "    qual_df['roa'] = qual_df['net_income'] / qual_df['total_assets']\n",
        "    qual_df['asset_turnover'] = qual_df['total_revenue'] / qual_df['total_assets']\n",
        "    qual_df['asset_turnover_chg'] = qual_df.groupby('ticker_symbol')['asset_turnover'].diff()\n",
        "    qual_df['accruals'] = (qual_df['net_income'] - qual_df['operating_income']) / qual_df['total_assets']\n",
        "    qual_df['leverage_ratio'] = qual_df['operating_income'] / qual_df['total_liabilities']\n",
        "\n",
        "    qual_df['accruals_norm'] = 1 - qual_df.groupby('period_ending')['accruals'].transform(\n",
        "        lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "    )\n",
        "\n",
        "    # Higher the better\n",
        "    for metric in ['roa', 'asset_turnover_chg', 'leverage_ratio']:\n",
        "        qual_df[metric + '_norm'] = qual_df.groupby('period_ending')[metric].transform(\n",
        "            lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "        )\n",
        "\n",
        "    qual_df['quality_score'] = qual_df[['roa_norm', 'asset_turnover_chg_norm', 'accruals_norm', 'leverage_ratio_norm']].mean(axis=1)\n",
        "    qual_df = qual_df.rename(columns={'period_ending': 'date', 'ticker_symbol': 'symbol'})\n",
        "    qual_df['date'] = pd.to_datetime(qual_df['date'])\n",
        "\n",
        "    return qual_df[['symbol', 'date', 'quality_score']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fi09xRpPIN-A",
      "metadata": {
        "id": "fi09xRpPIN-A"
      },
      "source": [
        "## ***Volume(Liquidity) Factor***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eh4km7zhIThM",
      "metadata": {
        "id": "Eh4km7zhIThM"
      },
      "source": [
        "***Idea:***  Liquid stocks are easier to trade and more stable.\n",
        "\n",
        "*'Liquid' means how easily and quickly an asset(like a stock) can be bought or sold in the market w/o significantly affecting it's price.*\n",
        "\n",
        "***Key Metrics:***\n",
        "  * **Average Daily Trading Volume**\n",
        "  * **Turnover Ratio -** Volume/ Shares Outstanding\n",
        "\n",
        "*High volume means there are high number of investor interested in a particular stock.*\n",
        "\n",
        "> We'll use volume as a filter to eliminate illiquid stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "wMdkPHslJudf",
      "metadata": {
        "id": "wMdkPHslJudf"
      },
      "outputs": [],
      "source": [
        "# Calculate Volume factor using On-Balance Volume (OBV)\n",
        "def calculate_volume_factor(prices_df):\n",
        "    vol_df = prices_df.copy()\n",
        "    vol_df['price_change'] = vol_df.groupby('symbol')['close'].diff()\n",
        "    vol_df['obv_direction'] = np.where(vol_df['price_change'] > 0, 1,\n",
        "                              np.where(vol_df['price_change'] < 0, -1, 0))\n",
        "    vol_df['obv'] = vol_df.groupby('symbol')['obv_direction'].transform(lambda x: (x * vol_df['volume']).cumsum())\n",
        "\n",
        "    vol_df['volume_score'] = vol_df.groupby('date')['obv'].transform(\n",
        "        lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "    )\n",
        "    vol_df['date'] = pd.to_datetime(vol_df['date'])\n",
        "    return vol_df[['symbol', 'date', 'volume_score']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gYcxPaB-Ju0A",
      "metadata": {
        "id": "gYcxPaB-Ju0A"
      },
      "source": [
        "## ***Volatility Factor***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5DcXYmweJzc1",
      "metadata": {
        "id": "5DcXYmweJzc1"
      },
      "source": [
        "***Idea:***  Stocks with lower price fluctuations tend to give better risk-adjusted returns.\n",
        "\n",
        "***Key Metrics:***\n",
        "  * **Standard Deviation of Daily Returns**\n",
        "  * **Beta -** Senstivity to market movements.\n",
        "\n",
        "*Many investors irrationally chase high-volatility \"lottery-like\" stocks, causing low-volatility stocks to be underpriced.*\n",
        "\n",
        "> We'll select stocks with the lowest historical volatility over 1â€“2 years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "r6LsXgimntgj",
      "metadata": {
        "id": "r6LsXgimntgj"
      },
      "outputs": [],
      "source": [
        "def calculate_volatility_factor(prices_df, window=30):\n",
        "    volat_df = prices_df.copy()\n",
        "    volat_df['daily_return'] = volat_df.groupby('symbol')['close'].pct_change()\n",
        "    volat_df['volatility'] = volat_df.groupby('symbol')['daily_return'].transform(lambda x: x.rolling(window).std())\n",
        "    volat_df['date'] = pd.to_datetime(volat_df['date'])\n",
        "\n",
        "    # Normalize: lower volatility = higher score\n",
        "    def normalize_group(x):\n",
        "        if x.max() == x.min():\n",
        "            return pd.Series([0.5]*len(x), index=x.index)\n",
        "        return 1 - (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "    volat_df['volatility_score'] = volat_df.groupby('date')['volatility'].transform(normalize_group)\n",
        "\n",
        "    # Fill any remaining NaNs with 0.5 (neutral)\n",
        "    volat_df['volatility_score'] = volat_df['volatility_score'].fillna(0.5)\n",
        "    return volat_df[['symbol', 'date', 'volatility_score']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e1229f",
      "metadata": {},
      "source": [
        "## ***NLP Integration (Sentiment Factor)***\n",
        "The NLP is used to do the following things -\n",
        "1. Uses FinBERT model to score the sentiment from financial texts,\n",
        "2. Scores financial news headlines or tweets,\n",
        "3. Computes a `sentiment_factor = positive - negative`,\n",
        "4. Aggregates sentiment scores per stock symbol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e86de9",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "model.eval()\n",
        "\n",
        "# Labels used for the model\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "\n",
        "# Scores sentiment of a single news/tweet string.\n",
        "def score_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1).numpy()[0]\n",
        "    return dict(zip(labels, probs))\n",
        "\n",
        "# Adds FinBERT sentiment scores to a DataFrame of financial news/tweets.\n",
        "def compute_sentiment_scores(news_df):\n",
        "    sentiment_data = news_df['text'].apply(score_sentiment).apply(pd.Series)\n",
        "    return pd.concat([news_df, sentiment_data], axis=1)\n",
        "\n",
        "# Aggregates sentiment scores into a sentiment factor by averaging per (date, symbol).\n",
        "def get_sentiment_factor(news_df):\n",
        "    df = compute_sentiment_scores(news_df)\n",
        "    df['sentiment_factor'] = df['positive'] - df['negative']  # Customize as needed\n",
        "    sentiment_factor = df.groupby(['date', 'symbol'])['sentiment_factor'].mean().reset_index()\n",
        "    return sentiment_factor\n",
        "\n",
        "# Merging the sentiment_factor_df with the factor scores.\n",
        "# Function to merge sentiment factor with existing factor scores\n",
        "def merge_sentiment_with_factors(factor_scores_df, sentiment_df):\n",
        "    merged_df = factor_scores_df.merge(\n",
        "        sentiment_df,\n",
        "        on=['date', 'symbol'],\n",
        "        how='left'\n",
        "    )\n",
        "    merged_df['sentiment_factor'] = merged_df['sentiment_factor'].fillna(0)  # Handle missing values\n",
        "    return merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f067345",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "lXAtuKX8SLFD",
      "metadata": {
        "id": "lXAtuKX8SLFD"
      },
      "source": [
        "## ***Calculating the Factor Values***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "D8M7nNqw1m0v",
      "metadata": {
        "id": "D8M7nNqw1m0v"
      },
      "outputs": [],
      "source": [
        "value_scores = calculate_value_factor(fundamentals, prices)\n",
        "quality_scores = calculate_quality_factor(fundamentals)\n",
        "momentum_scores = calculate_momentum_factor(prices)\n",
        "volume_scores = calculate_volume_factor(prices)\n",
        "volatility_scores = calculate_volatility_factor(prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "q3VH0ifwW5sG",
      "metadata": {
        "id": "q3VH0ifwW5sG"
      },
      "outputs": [],
      "source": [
        "quality_scores['date'] = pd.to_datetime(quality_scores['date'])\n",
        "quality_scores_cleaned = quality_scores.sort_values('date').drop_duplicates(subset=['symbol', 'date'], keep='last')\n",
        "quality_scores_daily = quality_scores_cleaned.set_index('date').groupby('symbol').resample('D').ffill().drop(columns='symbol').reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Nc6s9MNm1fq5",
      "metadata": {
        "id": "Nc6s9MNm1fq5"
      },
      "source": [
        "## ***Combining All Factors (Composite Factor)***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V-PjCwYb1fq6",
      "metadata": {
        "id": "V-PjCwYb1fq6"
      },
      "source": [
        "While combining the factors, we will consider the following points -\n",
        "* ***Normalization -*** Min-max scaling ensures comparable factor scores.\n",
        "* ***Weight Customization -*** Adjust factor weights in composite score based on strategy.\n",
        "* ***Rebalancing Frequency -*** Monthly rebalancing recommended for factor strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jd4R4je7f7ht",
      "metadata": {
        "id": "Jd4R4je7f7ht"
      },
      "outputs": [],
      "source": [
        "def calculate_composite_score(factor_dfs, sentiment_df, sentiment_weight=0.1):\n",
        "    base_weights = {\n",
        "        'value': 0.18,\n",
        "        'momentum': 0.18,\n",
        "        'quality': 0.18,\n",
        "        'volume': 0.18,\n",
        "        'volatility': 0.18\n",
        "    }\n",
        "    total_base = sum(base_weights.values())\n",
        "    adjusted_weights = {k: v * (1 - sentiment_weight) / total_base for k, v in base_weights.items()}\n",
        "    adjusted_weights['sentiment'] = sentiment_weight\n",
        "\n",
        "    sentiment_df = sentiment_df.rename(columns={'sentiment_factor': 'sentiment_score'})\n",
        "    from functools import reduce\n",
        "\n",
        "    all_dfs = factor_dfs + [sentiment_df]\n",
        "    merged_df = reduce(lambda left, right: pd.merge(left, right, on=['symbol', 'date'], how='inner'), all_dfs)\n",
        "    merged_df['composite_score'] = (\n",
        "        adjusted_weights['value'] * merged_df['value_score'] +\n",
        "        adjusted_weights['momentum'] * merged_df['momentum_score'] +\n",
        "        adjusted_weights['quality'] * merged_df['quality_score'] +\n",
        "        adjusted_weights['volume'] * merged_df['volume_score'] +\n",
        "        adjusted_weights['volatility'] * merged_df['volatility_score'] +\n",
        "        adjusted_weights['sentiment'] * merged_df['sentiment_score']\n",
        "    )\n",
        "\n",
        "    return merged_df[['symbol', 'date', 'composite_score']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "17_QTMxxT6SK",
      "metadata": {
        "id": "17_QTMxxT6SK"
      },
      "outputs": [],
      "source": [
        "factor_dataframes = {\n",
        "    'value': value_scores,\n",
        "    'momentum': momentum_scores,\n",
        "    'quality': quality_scores,\n",
        "    'volume': volume_scores,\n",
        "    'volatility': volatility_scores\n",
        "}\n",
        "composite_scores = calculate_composite_score(factor_dataframes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b73aba1",
      "metadata": {},
      "outputs": [],
      "source": [
        "composite_scores = calculate_composite_score_with_sentiment(\n",
        "    factor_dataframes,\n",
        "    sentiment_df=sentiment_scores,  \n",
        "    sentiment_weight=0.15          \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ijNaJLg1Ts_X",
      "metadata": {
        "id": "ijNaJLg1Ts_X"
      },
      "outputs": [],
      "source": [
        "value_scores.to_csv('value_scores.csv', index=False)\n",
        "momentum_scores.to_csv('momentum_scores.csv', index=False)\n",
        "quality_scores.to_csv('quality_scores.csv', index=False)\n",
        "volume_scores.to_csv('volume_scores.csv', index=False)\n",
        "volatility_scores.to_csv('volatility_scores.csv', index=False)\n",
        "composite_scores.to_csv('composite_scores.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
